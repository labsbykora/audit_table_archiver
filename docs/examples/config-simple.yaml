# Example configuration file for the archiver tool

version: "2.0"  # Config file version

# S3 Configuration section: settings for backup storage
s3:
  endpoint: null  # S3 service root URL; null for AWS S3, or e.g. https://s3.example.com for S3-compatible services
  bucket: audit-archives  # The name of the S3 bucket to use for storing archives
  prefix: archives/  # S3 key prefix for all archived files (can organize by folders)
  region: us-east-1  # AWS region or region for the S3-compatible API
  storage_class: STANDARD_IA  # S3 storage class (e.g., STANDARD, STANDARD_IA, GLACIER)
  encryption: SSE-S3  # S3 server-side encryption type (SSE-S3, SSE-KMS, etc.)
  multipart_threshold_mb: 10  # Use multipart uploads for files larger than this size (in MB)
  # Credentials settings (development only). For production, use environment variables:
  # access_key_id: your-access-key      # S3 access key (dev only; do not commit real keys)
  # secret_access_key: your-secret-key  # S3 secret access key (dev only)

# Global default options applied to all tables/databases unless overridden
defaults:
  retention_days: 90                 # Only archive records older than this many days (e.g., 90=archive anything older than 90 days)
  batch_size: 10000                  # Number of records archived per batch operation
  sleep_between_batches: 2           # Seconds to sleep between handling archive batches
  vacuum_after: true                 # Whether to run VACUUM after archive/delete
  vacuum_strategy: standard          # VACUUM type: standard (default), none, analyze, full (Postgres only)

# Databases to connect for archiving



databases:
  - name: production_db              # Name for this database connection group
    host: localhost                  # Hostname or IP of Postgres server
    port: 5432                       # Database port (default 5432 for Postgres)
    user: archiver                   # Username for authentication
    password_env: DB_PASSWORD        # Environment variable containing password (read from environment, not here)
    tables:
      - name: audit_logs             # Table name to archive
        schema: public               # Schema name for the table (default: public)
        timestamp_column: created_at # Name of column with timestamp used for retention filtering
        primary_key: id              # Primary key column (must uniquely identify rows)
        retention_days: 90           # Retention period for this table (can override global default)
        batch_size: 10000            # Batch size for this table (can override global default)

# --- Performance tuning settings ---

performance:
  # Adaptive batch processing (enable to let archiver auto-tune batch size for speed)
  adaptive_batch:
    enabled: true
    initial_batch_size: 10000
    min_batch_size: 1000
    max_batch_size: 50000
    target_query_time: 2.0   # Target duration (seconds) for archive query batches

  # Connection pool settings (applies to each database)
  connection_pool_size: 5   # Override per-database allowed

  # Parallel execution
  max_parallel_tables: 2    # How many tables to process in parallel per run (set higher for more throughput, beware DB load)

  # S3 upload concurrency
  s3_max_workers: 4         # How many multipart upload workers to use for S3 uploads (higher = faster, uses more memory)

# Optional: Configuration for restore watermark tracking
restore_watermark:
  enabled: true                    # Enable/disable watermark tracking for restore progress
  storage_type: "s3"               # Where to store watermark: "s3", "database", or "both"
  update_after_each_archive: true  # Whether to update watermark after every archive file, or only after all done

